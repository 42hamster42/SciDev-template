{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33a7d46-d93f-40b0-8256-97a1c43344e6",
   "metadata": {},
   "source": [
    "## Домашнее задание по свёрточным сетям"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbc722-2c7e-41a1-b25f-913c32228a2d",
   "metadata": {},
   "source": [
    "Сутью домашнего задания является последовательная реализация базовых операций, применяемых в свёрточных сетях с использованием операций над тензорами PyTorch, но без применения модулей torch.nn. Студенты должны самостоятельно реализовать как прямой и обратный проходы слоёв, так и классы нейронных сетей.\n",
    "\n",
    "Правильность выполнения задания будет проверяться идентичностью прохождения процесса обучения в тех же архитектурах, выполненных с применением модулей и алгоритмов PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af498e6-0c14-4731-a899-ad5f6b19766b",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78d40c-6d89-49d5-be7f-43f9e8607921",
   "metadata": {},
   "source": [
    "В первом задании требуется реализовать классы двухмерной свёртки **Conv**, линейного слоя **Fc**, алгоритм обучения нейронной сети **SGD**, функцию активации **ReLU**, **Softmax** и функцию эмпирического риска **CrossEntropyLoss**.\n",
    "\n",
    "Свёрточные и полносвязные слои должны реализовывать операцию сдвига (*bias*, *b*). \n",
    "Сверить формулы прямого прохода можно в документации по [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) и [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Далее, следует реализовать класс модели, который включает в себя реализованные выше компоненты. Требуется повторить параметры обучения и архитектуру, реализованную ниже с помощью torch.nn модулей. Критерием правильности решения будет совпадение значений эмпирического риска при обучении обеих реализаций сетей на одних и тех же данных, с теми же параметрами и с одинаковыми начальными инициализациями весов.\n",
    "\n",
    "Данные для обучения состоят из 4 примеров вертикальных и горизонтальных линий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a74f3a-578c-4ca9-aa1c-1834e19b7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "\n",
    "_ = torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a852825-7062-4829-a9e3-3e341cc46076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y, num_classes):\n",
    "    N = y.shape[0]\n",
    "    Z = torch.zeros((N, num_classes))\n",
    "    Z[torch.arange(N), y] = 1\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94731e-4897-4ad1-96a5-b128216b6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение будет вестись на следующих данных\n",
    "def get_data():\n",
    "    # Задача классификации вертикальных и горизонтальных линий\n",
    "    # Данные\n",
    "    vert1 = [[0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 0, 250, 0, 0]]\n",
    "\n",
    "    vert2 = [[0, 0, 220, 40, 0],\n",
    "             [0, 0, 250, 10, 0],\n",
    "             [0, 0, 250, 0, 0],\n",
    "             [0, 10, 250, 0, 0],\n",
    "             [0, 40, 220, 0, 0]]\n",
    "\n",
    "    hor1 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [250, 250, 250, 250, 250],\n",
    "            [0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    hor2 = [[0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 20],\n",
    "            [220, 250, 250, 250, 200],\n",
    "            [10, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0]]\n",
    "\n",
    "    data = [vert1, vert2, hor1, hor2]\n",
    "    labels = [0, 0, 1, 1]\n",
    "    \n",
    "    train_x = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return train_x, labels\n",
    "\n",
    "train_x, labels = get_data()\n",
    "print(f'Train data shape [Batch, Channels, Height, Width] = {train_x.shape}')\n",
    "print(f'Labels shape = {labels.shape}')\n",
    "\n",
    "\n",
    "train_mean = torch.mean(train_x)\n",
    "train_std = torch.std(train_x)\n",
    "batch = (train_x - train_mean) / train_std\n",
    "\n",
    "print(f'Mean and standard deviation before normalization = {train_mean.item():.2f}, {train_std.item():.2f}')\n",
    "print(f'Mean and standard deviation after normalization = {torch.mean(batch).item():.2f}, {torch.std(batch).item():.2f}')\n",
    "\n",
    "print('Train batch')\n",
    "for img in batch:\n",
    "    plt.imshow(img.squeeze(0), cmap='Greys')  # Цвета инвертированы. Чем темнее, тем значение пикселя больше\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f2d43-d5d9-46cb-92f4-1b55e8870dbe",
   "metadata": {},
   "source": [
    "Реализуем эталонную модель **TorchGradientModel**, состоящую из следующих модулей:\n",
    "- Сверточный слой с 4 фильтрами размера $5\\times5$;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Линейный слой с 2 выходными нейронами для классов горизонтальной и вертикальной линий\n",
    "\n",
    "Вопрос:\n",
    "Каким ещё образом можно осуществить бинарную классификацию, не используя линейный слой с 2 выходными нейронами?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f54ef4-1d73-4e63-ba2b-f904df6534cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78879f-cbf9-41d5-98d3-806367c5b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchGradientModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=5, padding=0, bias=True)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * 1 * 1, 2)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c48edb-7561-46c0-a045-f739adb76b97",
   "metadata": {},
   "source": [
    "Дополнительная информация может быть найдена в комментариях к коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dedb7-e1ce-4283-bd3c-650e3d5aca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данная сеть обучается за 3 эпохи, что удобно для процесса отладки.\n",
    "learning_rate = 1\n",
    "epochs = 3\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "# torch_model_params[0] - тензор с весами фильтров W свёрточного слоя\n",
    "# torch_model_params[1] - тензор с весами сдвигов b свёрточного слоя\n",
    "# torch_model_params[2] - тензор с весами W линейного слоя\n",
    "# torch_model_params[3] - тензор с весами сдвигов b линейного слоя\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (для I-го слоя - это кол-во изображений),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для данного эксперимента используется самый простой алгоритм обучения без моментов.\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1425e7-8003-42e7-bac1-7f13d7852a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "    # Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605478d-3157-4556-a5af-3fae08b02da1",
   "metadata": {},
   "source": [
    "Ниже представлены заготовки (шаблоны) классов, колторые требуется реализовать. **Conv**, **Fc** и **ReLU** должны иметь методы **forward** и **backward** для прямого и обратного прохода по сети. При прямом проходе следует кэшировать данные, которые потребуются для вычисления градиента.\n",
    "\n",
    "**Свёртки**\n",
    "\n",
    "При прямом проходе нужно брать фильтры поочерёдно и проводить свёртку со входным тензором. Каждая операция свёртки даёт 2-мерную матрицу на выходе. Для получения итоговой карты признаков следует сконкатенировать эти матрицы, чтобы получить тензор рамерами [Размер батча, Количество каналов, Высота, Ширина]. Реализовывать можно как с помощью вложенных циклов, так и с применением векторизации. В данном задании важно не время работы, но точность вычислений.\n",
    "\n",
    "В первом модуле заданий уже была показана реализация обратного прохода по линейному слою для подсчёта частных производных по эмпирическому риску, которые использовались для обновления весов слоя. Расчёт частных производных в свёрточных слоях идеологически тот же. Требуется посчитать частные производные по dX предыдущему входу слоя, dW по весам фильтров и db по сдвигам. Пусть dZ – это градиент ошибки к выхожу текущего свёрточного слоя (передаётся от предыдущего слоя при обратном проходе сети), тогда\n",
    "\n",
    "$dX += \\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W} W_c\\times dZ_{hw}$,\n",
    "\n",
    "где $W_c$ – это фильтр, а $dZ_{hw}$ – скаляр, соответствующий градиенту эмпирического риска к выходу текущего свёрточного слоя $Z$ в $n$ строке и $w$ столбце. Так как при прямом проходе фильтр $W_c$ влияет на все значения канала с карты признаков, то мы умножаем один и тот же фильтр $W_c$ с разными $dZ$ в пределах канала $с$, суммируя результаты.\n",
    "В numpy эта операция выглядела бы так\n",
    "\n",
    " dX[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    " \n",
    "В PyTorch очерёдность каналов иная: [Batch, Channel, Height, Width].\n",
    "\n",
    "Производная одного фильтра относительно эмпирического риска  считается по формуле\n",
    "\n",
    "$dW_c+=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}x_{slice} \\times dZ_{hw}$,\n",
    "\n",
    "где $x_{slice}$ относится к отрезку из входного тензора, который был использован в прямом проходе, чтобы получить активацию $Z_{ij}$. Таким образом мы получим градиент фильтра $W$ относительно данного отрезка. Так как в рамках свёрточного слоя для разных отрезков мы использовали тот же фильтр $W$, то мы складываем эти градиенты, чтобы получить $dW$.\n",
    "\n",
    "В numpy подобная операция реализуется так:\n",
    "\n",
    "dW[:,:,:,c] += x_slice * dZ[i, h, w, c]\n",
    "\n",
    "Производная по сдвигам считается как сумма всех градиентов выхода свёрточного слоя:\n",
    "\n",
    "$db=\\sum_{h=0}^{n_H}\\sum_{w=0}^{n_W}dZ_{hw}$\n",
    "\n",
    "В numpy реализовывалась бы так:\n",
    "\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "\n",
    "\n",
    "Для реализации выполнения обратного прохода градиента идентичным nn.CrossEntropyLoss(reduction='mean') образом, значения $dW$ и $db$ следует делить на размер пакета (batch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ef530-74d6-4ab3-b935-d5766410aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наивная реализация свёртки, медленная\n",
    "class Conv():\n",
    "    def __init__(self, nb_filters: int, filter_size: int, nb_channels: int, stride: int = 1, padding: int = 0, sanity_check: bool = True):\n",
    "        self.num_filters = nb_filters\n",
    "        self.f = filter_size\n",
    "        self.n_C = nb_channels\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "        self.sanity_check = sanity_check  # использовать не обязательно\n",
    "\n",
    "        self.cache = None  # Для хранения данных прямого прохода сети, которые потребуются при обратном проходе\n",
    "        \n",
    "        self.W = torch_model_params[0]\n",
    "        self.dW = torch.zeros(self.W.shape)\n",
    "\n",
    "        \n",
    "        self.b = torch_model_params[1]\n",
    "        self.db =  torch.zeros(self.b.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def single_conv(img, w, b):\n",
    "        # Поэлементное произведение\n",
    "        s = torch.multiply(img, w)\n",
    "        # сумма произведений\n",
    "        g = torch.sum(s)\n",
    "        # сдвиг\n",
    "        g = g + b\n",
    "        return g\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Прямой проход\n",
    "        - X : выход предыдущего свёрточного слоя с размерностями (m, n_C_prev, n_H_prev, n_W_prev).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Распространяет градиент ошибки от предыдущего слоя в текущий свёрточный слой\n",
    "        - dZ: ошибка из предыдущего слоя (по направлению от выхода ко входу сети).\n",
    "            \n",
    "        Возвращает:\n",
    "        - dX: ошибка текущего свёрточного слоя.\n",
    "        - self.dW: градиент по весам.\n",
    "        - self.db: градиент по сдвигам.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class Fc():\n",
    "    def __init__(self, row, column):\n",
    "        self.row = row\n",
    "        self.col = column\n",
    "\n",
    "        self.W = torch_model_params[2]\n",
    "        self.dW = 0\n",
    "        self.b = torch_model_params[3]\n",
    "        self.db = 0\n",
    "        \n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, fc):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        # не забываем, что для эмуляции CrossEntropyLoss(reduction='mean') нужно делить self.dW и self.db на размер пакета\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, lr, params):\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "\n",
    "    def update_params(self, grads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ReLU():\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, new_deltaL):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "class CrossEntropyLoss():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sm = Softmax() # Softmax не нужен в последнем слое модели сети, если вызывать в расчёте кросс-энтропии\n",
    "        pass\n",
    "    \n",
    "    def get(self, y_pred, y):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b894b-cf32-40cf-84e0-d172f23436fe",
   "metadata": {},
   "source": [
    "Реализовать модель, идентичную **TorchGradientModel**. \n",
    "\n",
    "Если есть желание переиспользовать код из первого модуля заданий по линейным слоям, то можно реализовать модель, пользуясь предыдущими абстрактными классами и импортировать их здесь, переписав код для проверки результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7233e2-c485-45f0-b33f-3e3de37741c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel():\n",
    "\n",
    "    def __init__(self, input_size = 5, num_classes = 2):\n",
    "        # ВПИСАТЬ МОДУЛИ СЕТИ ЗДЕСЬ\n",
    "        \n",
    "        self.layers = [self.conv1, self.fc1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        # ВПИСАТЬ ОБРАТНОЕ РАСПРОСТРАНЕНИЕ ОШИБКИ\n",
    "        \n",
    "        grads = { \n",
    "                'dW1': dW1, 'db1': db1,\n",
    "                'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "        \n",
    "        return grads\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W\n",
    "            params['b' + str(i+1)] = layer.b\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W = params['W'+ str(i+1)]\n",
    "            layer.b = params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc89670-2027-4321-abbb-a3f5365054ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel(input_size = X.shape[-1], num_classes=num_classes)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr = learning_rate, params = model.get_params())      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0 \n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item() / X.shape[0]\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60e2ae-3472-4d16-ad2f-076dd35f839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_model(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591454a2-a616-49c7-9dd1-18d323492924",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 1 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570f1c4-26d0-4d56-9b8c-79d70773f7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 1 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 1 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task1 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e52a3a-fde2-401b-9517-b41492d445b9",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7295ab-2cf9-4eac-b525-62f3777bd4fa",
   "metadata": {},
   "source": [
    "Требуется реализовать классы пакетной нормализации **BatchNorm2d** и слоя выборки усреднением **AvgPool**. Реализовать класс модели, идентичной эталонной TorchGradientModel2\n",
    "\n",
    "Эталонная модель **TorchGradientModel2**, состоить из:\n",
    "- Сверточный слой с 4 фильтрами размера $3\\times3$;\n",
    "- Пакетная нормализация;\n",
    "- Функция активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Выборки усреднением с ашгом 2 и размером окна 2\n",
    "- Линейный слой с переменных количеством выходных нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82edce-6429-4fd5-957a-d373e8ed46a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchGradientModel2(nn.Module):\n",
    "    def __init__(self, num_classes=2, input_size=5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=0, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4 * int((input_size - 2) / 2) * int((input_size - 2) / 2), num_classes)\n",
    "        # PyTorch автоматически применяет LogSoftmax при использовании CrossEntropyloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b11589-27fa-4201-ab3f-079ba27c8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2()\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. При желании можете поэкспериментировать с другими, которые реализовали для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(batch) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, labels)\n",
    "    loss_history.loc[e, 'loss_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item()\n",
    "\n",
    "    train_acc /= batch.shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a7ef0-9f4d-440c-8a6c-eaa9f884eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm2d():\n",
    "    def __init__(self, num_channels, gamma=1, beta=0, eps=1e-20):\n",
    "        self.num_channels = num_channels\n",
    "        # Применяем стандартные название полей для обновления весов, чтобы не переписывать код модели и оптимизатора\n",
    "        self.W = torch.ones(num_channels)  # gamma\n",
    "        self.b = torch.zeros(num_channels)  # beta\n",
    "        self.eps = eps\n",
    "        \n",
    "        self.dW = torch.zeros(num_channels)\n",
    "        self.db = torch.zeros(num_channels)\n",
    "\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, debug=True):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AvgPool():\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.f = filter_size\n",
    "        self.s = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, dout):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77309d-ad2e-46e8-9ffd-4e157006ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel2():\n",
    "\n",
    "    def __init__(self, input_size = 5, num_classes = 2):\n",
    "        # ВПИСАТЬ МОДУЛИ СЕТИ ЗДЕСЬ\n",
    "        \n",
    "        self.layers = [self.conv1, self.bn1, self.fc1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, deltaL):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params['W' + str(i+1)] = layer.W\n",
    "            params['b' + str(i+1)] = layer.b\n",
    "\n",
    "        return params\n",
    "\n",
    "    def set_params(self, params):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.W = params['W'+ str(i+1)]\n",
    "            layer.b = params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651e71c-2a12-45c6-8f05-32c23fca32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_model2(X, y, epochs, num_classes):\n",
    "    y = one_hot_encoding(labels, num_classes) # преобразуем число в эталоне в унитарный код.\n",
    "    \n",
    "    model = CustomModel2(input_size = X.shape[-1], num_classes=num_classes)\n",
    "    cost = CrossEntropyLoss()\n",
    "    \n",
    "    params = model.get_params()\n",
    "\n",
    "    optimizer = SGD(lr = learning_rate, params = model.get_params())      \n",
    "\n",
    "    t = trange(epochs)\n",
    "    \n",
    "    for e in t:\n",
    "        train_loss = 0\n",
    "        train_acc = 0 \n",
    "  \n",
    "        y_pred = model.forward(X)\n",
    "        loss, deltaL = cost.get(y_pred, y)\n",
    "        grads = model.backward(deltaL)\n",
    "        params = optimizer.update_params(grads)\n",
    "        model.set_params(params)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc = torch.sum(torch.argmax(predict_y, axis=1) == labels).item() / X.shape[0]\n",
    "        \n",
    "        loss_history.loc[e, 'loss_custom'] = train_loss\n",
    "        t.set_postfix(loss=train_loss, acc=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7053a57-ae3b-4f05-a73a-7d8b28079479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_model2(batch, labels, epochs, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d4eb9-b1ed-4919-8d37-98f12cbae3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df2684-a0a0-4025-8713-01e696c4827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute((loss_history.iloc[:,0].values-loss_history.iloc[:,1].values)).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 2 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 2 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cb689-abb1-458f-b851-046583c29220",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71114630-0f89-4b70-8c7c-cbe5091f0440",
   "metadata": {},
   "source": [
    "Обучить ранее реализованную сеть **CustomModel2** на реальных данных из выборки digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96e6f0-3d16-4656-aaf9-299c5cd423a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154aa79a-730a-4b53-9a6d-7769e5ab164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_numbers():\n",
    "    # Загружаем выборку digits\n",
    "    digits = load_digits(n_class=10, return_X_y=False, as_frame=False)\n",
    "\n",
    "    # ПРОВЕСТИ НЕОБХОДИМЫЕ ПРЕОБРАЗОВАНИЯ\n",
    "    \n",
    "    return train_x, train_label, val_x, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e861b8-49e4-4800-872a-c8ac7ee4093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_label, val_x, val_label = get_data_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766a8b7-dc86-4572-baa0-ecf80dec5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epochs = 10\n",
    "num_classes = 10\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Сохраняйте историю эмпирического риска каждую эпоху в отдельном столбце loss_history 'loss_custom'\n",
    "loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "\n",
    "torch_grad_model = TorchGradientModel2(num_classes, input_size=8)\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# Используйте эти веса, чтобы инициализировать веса своей сети для точной воспроизводимости результатов\n",
    "torch_model_params = []\n",
    "temp_m = copy.deepcopy(torch_grad_model)\n",
    "for param in temp_m.named_parameters():\n",
    "    if param[0] in ('conv1.weight', 'conv1.bias', 'fc1.weight', 'fc1.bias'):\n",
    "#         print(param)\n",
    "        torch_model_params.append(param[1].clone().detach())\n",
    "\n",
    "# СТУДЕНТАМ:\n",
    "# При реализации своих слоёв не забывайте делить получившиеся градиенты по ошибкам на размер пакета (кол-во изображений для I-го слоя),\n",
    "# чтобы эмулировать поведение CrossEntropyLoss с параметром reduction='mean'\n",
    "ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Для эксперимента используется самый простой оптимизатор. При желании можете поэкспериментировать с другими, которые реализовали для 1-го задания\n",
    "optimizer = torch.optim.SGD(torch_grad_model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "torch_grad_model.train()\n",
    "t = trange(epochs)\n",
    "loss_hist_pt = []\n",
    "for e in t:\n",
    "    predict_y = torch_grad_model(train_x[0:10]) # для обучения используем весь пакет\n",
    "    \n",
    "#     Можете выводить веса сети для прямого сравнения со своей реализацей, \n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param)\n",
    "\n",
    "    loss = ce(predict_y, train_label[0:10])\n",
    "    loss_history.loc[e, 'loss_train_pt'] = loss.item()\n",
    "    \n",
    "    # Градиенты нужно обнулять в каждой эпохе\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Градиенты так же можно выводить в текстовом виде для оценки хода обучения\n",
    "#     print('Gradients')\n",
    "#     for param in torch_grad_model.named_parameters():\n",
    "#         print(param[0], param[1].grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc = torch.sum(torch.argmax(predict_y, axis=1) == train_label[0:10]).item()\n",
    "\n",
    "    train_acc /= train_x[0:10].shape[0]\n",
    "    t.set_postfix(loss=loss.item(), accuracy=train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123a514-c557-44f2-baed-77f6b4adf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, learning_rate, loss_history, train_x, train_label, val_x, val_label):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e12010-8508-4c62-bf6d-e26cb66c62f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = pd.DataFrame(index=range(epochs), dtype=float)\n",
    "train_model(epochs, learning_rate, loss_history, train_x[0:10], train_label[0:10], val_x[0:10], val_label[0:10])\n",
    "\n",
    "f = plt.figure()\n",
    "plt.title('Task 2 train loss', color='black')\n",
    "loss_history.plot(ax=f.gca())\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603aeed-717d-4d6e-ad07-a652a284e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эмпирический риск на всех эпохах должен совпадать с реализацией PyTorch\n",
    "error = np.absolute(loss_history[['loss_train_custom']].to_numpy()-loss_history[['loss_train_pt']].to_numpy()).sum()\n",
    "if error < 1e-3:\n",
    "    print('Задание 3 выполнено успешно')\n",
    "else:\n",
    "    print('Задание 3 не выполнено. Эмпирический риск не совпадает в реализациях PyTorch и собственной')\n",
    "print(f'Суммарная ошибка = {error:.4f}')\n",
    "loss_history_task2 = loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a740226-459e-46c6-92ab-c38dac6c3468",
   "metadata": {},
   "source": [
    "### Необязательное задание\n",
    "Создать свою архитектуру свёрточной сети и обучиться на полной выборке digits, получив высокое качество классификации. Сравнить скорость и финальное качество обучения при применении разных алгоритмов обучения (SGD с моментом, Adam), регуляризации весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac56e4-981f-46b3-9ffd-54e63054f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
